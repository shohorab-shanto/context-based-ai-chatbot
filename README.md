# ğŸ§  LangChain Chatbot with PDF Knowledge & Memory (FastAPI)

This is a memory-aware chatbot application powered by **LangChain**, **FAISS**, and **FastAPI**. It loads knowledge from two PDFs, builds a vector store, and allows users to interact with the content via a contextual chatbot interface. Chat history is preserved per session and can be reset easily.

---

## ğŸš€ Features

- Load multiple PDFs using `PyPDFLoader`
- Chunk text and embed using `HuggingFaceEmbeddings`
- Store vectors in `FAISS` for fast retrieval
- Ask follow-up or contextual questions with LangChainâ€™s memory-enabled RAG pipeline
- Reset chat history via a simple UI button
- Lightweight frontend with HTML + Jinja2 templates

---

## ğŸ“ Project Structure

ğŸ“¦ langchain-pdf-chatbot/
â”œâ”€â”€ app.py # Main FastAPI app
â”œâ”€â”€ templates/
â”‚ â””â”€â”€ index.html # Chat UI template
â”œâ”€â”€ static/ # Optional static folder for CSS/JS
â”œâ”€â”€ pdfs/
â”‚ â”œâ”€â”€ topic1.pdf
â”‚ â””â”€â”€ topic2.pdf
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


---

## ğŸ› ï¸ Requirements

- Python 3.8+
- pip

---

## ğŸ“¦ Installation


git clone https://github.com/your-username/langchain-pdf-chatbot.git
cd langchain-pdf-chatbot
python -m venv venv
source venv/bin/activate

## For Local setup command
pip install fastapi uvicorn jinja2 aiofiles      
pip install langchain langchain-openai langchain-community langchain-core faiss-cpu sentence-transformers unstructured pypdf

## PDF Setup
Place your two PDFs in the pdfs/ directory and name them:

topic1.pdf
topic2.pdf

These will be used as the knowledge base for the chatbot.

ğŸ”‘ Configure LLM
This uses an OpenAI-compatible endpoint. You must set a token (e.g., from GitHub AI or similar):

os.environ['GITHUB_TOKEN'] = "your-token-here"

## Run the app:
uvicorn app:app --reload
